{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution\n",
    "This notebook tries to repeat the result of [this article](https://arxiv.org/pdf/1511.04587.pdf \"Accurate Image Super-Resolution Using Very Deep Convolutional Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import caffe\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from PIL import Image\n",
    "\n",
    "in_db = lmdb.open('image-lmdb', map_size=int(1e12))\n",
    "with in_db.begin(write=True) as in_txn:\n",
    "    for in_idx, in_ in enumerate(inputs):\n",
    "        # load image:\n",
    "        # - as np.uint8 {0, ..., 255}\n",
    "        # - in BGR (switch from RGB)\n",
    "        # - in Channel x Height x Width order (switch from H x W x C)\n",
    "        im = np.array(Image.open(in_)) # or load whatever ndarray you need\n",
    "        im = im[:,:,::-1]\n",
    "        im = im.transpose((2,0,1))\n",
    "        im_dat = caffe.io.array_to_datum(im)\n",
    "        in_txn.put('{:0>10d}'.format(in_idx), im_dat.SerializeToString())\n",
    "in_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def srcnn(lmdb, batch_size, mean_file='abc'):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.residue = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb, transform_param=dict(mean_file=mean_file), ntop=2)\n",
    "\n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=3,num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu1 = L.ReLU(n.conv1, in_place=True)\n",
    "    n.conv2 = L.Convolution(n.relu1, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu2 = L.ReLU(n.conv2, in_place=True)\n",
    "    n.conv3 = L.Convolution(n.relu2, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu3 = L.ReLU(n.conv3, in_place=True)\n",
    "    n.conv4 = L.Convolution(n.relu3, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu4 = L.ReLU(n.conv4, in_place=True)\n",
    "    n.conv5 = L.Convolution(n.relu4, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu5 = L.ReLU(n.conv5, in_place=True)\n",
    "    n.conv6 = L.Convolution(n.relu5, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu6 = L.ReLU(n.conv6, in_place=True)\n",
    "    n.conv7 = L.Convolution(n.relu6, kernel_size=3,num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu7 = L.ReLU(n.conv7, in_place=True)\n",
    "    n.conv8 = L.Convolution(n.relu7, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu8 = L.ReLU(n.conv8, in_place=True)\n",
    "    n.conv9 = L.Convolution(n.relu8, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu9 = L.ReLU(n.conv9, in_place=True)\n",
    "    n.conv10 = L.Convolution(n.relu9, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu10 = L.ReLU(n.conv10, in_place=True)\n",
    "    n.conv11 = L.Convolution(n.relu10, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu11 = L.ReLU(n.conv11, in_place=True)\n",
    "    n.conv12 = L.Convolution(n.relu11, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu12 = L.ReLU(n.conv12, in_place=True)\n",
    "    n.conv13 = L.Convolution(n.relu12, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu13 = L.ReLU(n.conv13, in_place=True)\n",
    "    n.conv14 = L.Convolution(n.relu13, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu14 = L.ReLU(n.conv14, in_place=True)\n",
    "    n.conv15 = L.Convolution(n.relu14, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu15 = L.ReLU(n.conv15, in_place=True)\n",
    "    n.conv16 = L.Convolution(n.relu15, kernel_size=3,num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu16 = L.ReLU(n.conv16, in_place=True)\n",
    "    n.conv17 = L.Convolution(n.relu16, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu17 = L.ReLU(n.conv17, in_place=True)\n",
    "    n.conv18 = L.Convolution(n.relu17, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu18 = L.ReLU(n.conv18, in_place=True)\n",
    "    n.conv19 = L.Convolution(n.relu18, kernel_size=3, num_output=64, weight_filler=dict(type='gaussian', std=0.01))\n",
    "    n.relu19 = L.ReLU(n.conv19, in_place=True)\n",
    "    n.conv20 = L.Convolution(n.relu19, kernel_size=3, num_output=1, weight_filler=dict(type='gaussian', std=0.01))\n",
    "\n",
    "    n.loss =  L.Euclidian(n.conv20, n.residue)\n",
    "    return n.to_proto()\n",
    "\n",
    "# Important parameters!\n",
    "training_set_size = 50000\n",
    "testing_set_size = 10000\n",
    "train_batch_size = 150\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('srcnn_auto_train.prototxt', 'w+') as f:\n",
    "    f.write(str(srcnn('srcnn_train', train_batch, 'means/srcnn_train_mean.binaryproto')))\n",
    "    \n",
    "with open('srcnn_auto_test.prototxt', 'w+') as f:\n",
    "    f.write(str(srcnn('srcnn_test', test_batch, 'means/srcnn_test_mean.binaryproto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat srcnn_auto_train.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat srcnn_auto_train.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat srcnn_auto_solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize solver\n",
    "solver = caffe.SGDSolver('srcnn_auto_solver.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore model from iteration x\n",
    "solver.net.copy_from('breakpoints/toX.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the structure of the model for the first time\n",
    "model_def = 'task2/alexnet_auto_train.prototxt'\n",
    "net = caffe.Net(model_def,\n",
    "                caffe.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plt.figure(num=1, figsize=(48,24), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot()\n",
    "\n",
    "niter = 20000\n",
    "test_interval = 500\n",
    "# losses will also be stored in the log\n",
    "train_loss = np.zeros(niter)\n",
    "test_loss = np.zeros(int(np.ceil(niter / test_interval)))\n",
    "pictures_number = niter / test_interval\n",
    "rowsnum = 4\n",
    "picidx = 0\n",
    "# the main solver loop\n",
    "for it in range(niter):\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    # store the train loss\n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    \n",
    "    # run a full test every so often\n",
    "    # (Caffe can also do this for us and write to a log, but we show here\n",
    "    #  how to do it directly in Python, where more complicated things are easier.)\n",
    "    if it % test_interval == 0:\n",
    "        picidx += 1\n",
    "        print 'Iteration', it, 'testing...'\n",
    "        score = 0.0\n",
    "        for test_it in range(test_set_size / test_batch_size):\n",
    "            solver.test_nets[0].forward()\n",
    "            score = solver.test_nets[0].blobs['loss'].data\n",
    "        test_loss[it / test_interval] = score\n",
    "        \n",
    "        # plt middle results\n",
    "        plt.subplot(rowsnum, pictures_number/rowsnum, picidx)\n",
    "        plt.grid(True)\n",
    "        plt.plot(np.arange(niter), train_loss)\n",
    "        plt.plot(test_interval * np.arange(len(test_score)), test_acc, 'r')\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('loss, iteration %d' % it)\n",
    "        plt.legend(['train', 'test'], loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# send results via email\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import smtplib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def noticeEMail(train_str, test_str, usr, psw, fromaddr, toaddr):\n",
    "    \"\"\"\n",
    "    Sends an email message through GMail once the script is completed.  \n",
    "    Developed to be used with AWS so that instances can be terminated \n",
    "    once a long job is done. Only works for those with GMail accounts.\n",
    "    \n",
    "    starttime : a datetime() object for when to start run time clock\n",
    "    usr : the GMail username, as a string\n",
    "    psw : the GMail password, as a string \n",
    "    \n",
    "    fromaddr : the email address the message will be from, as a string\n",
    "    \n",
    "    toaddr : a email address, or a list of addresses, to send the \n",
    "             message to\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize SMTP server\n",
    "    server=smtplib.SMTP('smtp.gmail.com:587')\n",
    "    server.starttls()\n",
    "    server.login(usr,psw)\n",
    "    \n",
    "    # Send email\n",
    "    senddate=datetime.strftime(datetime.now(), '%Y-%m-%d')\n",
    "    subject=\"Your job is complete\"\n",
    "    m=\"Date: %s\\r\\nFrom: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\nX-Mailer: My-Mail\\r\\n\\r\\n\" % (senddate, fromaddr, toaddr, subject)\n",
    "    msg='''\n",
    "    \n",
    "    train: ''' + train_str + ''' \n",
    "    \n",
    "    test ''' + test_str\n",
    "    \n",
    "    server.sendmail(fromaddr, toaddr, m+msg)\n",
    "    server.quit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    # Fill these in with the appropriate info...\n",
    "    usr='dem4064@gmail.com'\n",
    "    psw=''\n",
    "    fromaddr='dem4064@gmail.com'\n",
    "    toaddr='dmitriy.denisenko@phystech.edu'\n",
    "\n",
    "    # Send notification email\n",
    "    noticeEMail(np.array_str(train_loss), np.array_str(test_loss), usr, psw, fromaddr, toaddr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
